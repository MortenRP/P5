\section{Learning traffic patterns}\label{traffic-patterns}
% Why find traffic patterns?
% What is this section for?
% How? --> follows from the subsections.
In order to reason about the traffic in Beijing, and provide some measure of how the previous traffic possibly affects our belief in how the future traffic situation will be in the road network, we analyse the preprocessed data to look for patterns. In this section, we consider methods for extracting such patterns and propose how to apply them on the Beijing dataset.
\subsection{Methods}\label{patterns:methods}
From our knowledge representation of the road network and traffic, we want to predict how traffic affects the travel time of roads in the road network. In order to do this, we need a measure of the traffic on roads. There are several ways one could construct such a measure, including
\begin{itemize}
\item \emph{capacity}: How many vehicles can travel on a road, safely, while not negatively affecting the speed driven on the road?
\item \emph{classification}: Define a class feature of a road, that defines the state of the road. An example could be $Traffic \in \{free, light, medium, heavy\}$. A given road then has a classification based on the observations of speed within some time period.
\item \emph{speed}: we measure the traffic on a given road, by how fast you can drive on the road. In addition, one could measure the ratio between speed and speed limit.
\end{itemize}
The capacity of a road would be possible to estimate from the speed limit, length of a road, safety distances and number of cars. However, drivers do not always drive in a similar manner, which makes it difficult to accurately estimate capacities. Furthermore, one has to consider that different types of roads has different capacities based on number of lanes, differences in road material or other properties.

Performing some discrete classification could help to distinguish between different traffic states. Intuitively, a person might be interested in knowing if a road is clear, mildly congested or heavily congested. Such information could be captured by classifying roads by a ratio between speed limit and actual speed. This raises the questions of which classes should be used and how many. If there are not enough classes, it might be difficult to choose which road to take if both are congested more or less equally.\par

To more easily be able to evaluate roads where traffic is similar, we must use a measure that has a finer granularity. By looking directly at the actual speed driven on roads, we can determine more precisely the costs of taken a particular road. The actual speed, together with the length of a road is also the most important factors in in evaluating the travel time for a road. Therefore, we take a regression approach for predicting traffic.
\subsection{Regression model}\label{patterns:regression-model}
To predict the traffic speed on a given road segment, at a given time we can consider a speed prediction as a function:

\begin{align}\label{eq:speed}
speed: E \times T \times W \rightarrow \mathbb{R}
\end{align}
that maps a road segment, time of day and day of week to a real valued speed prediction. The remaining question is then, how to actually perform the prediction of traffic on a segment given a specific time. \par
Predicting the numeric value of some variable based on previous observations is typically associated with regression. Regression fits input and output pairs to a function, such that future values for input pairs can be predicted. Fitting a regression function on the observations for a particular road segment, provides a way to predict the traffic on the segment. However, since traffic varies over time, one could argue that a polynomial regression function would fit the observations better than a linear function as illustrated in \figref{fig:compare-regression}.

Even though the higher order regression functions generally fits the training data better, such a fit might result in \emph{overfitting}, where the accuracy of future predictions decreases and the linear regression might fit the future observations better.

\begin{figure}[h]
	\includegraphics[width=\textwidth]{figures/compare-regression.pdf}
	\caption{Different regression fits on the same dataset.}
	\label{fig:compare-regression}
\end{figure}

However, a linear regression over the observations for a whole day is not as useful for predicting the fluctuations in traffic during the day, because linear functions by definition cannot model large fluctuations. For this reason, we instead utilise several linear regression models over the course of a day. More specifically, we split a day $w$ into partitions $p_1,...,p_i,...,p_n$ where $p_i = (t_i,...,t_j,...,t_k)$ and $t_j$ are timestamps. Thus, a partition is defined as a period of time between $t_i$ and $t_k$ that contains every possible timestamp $t_j$ from $t_i$ to $t_k$. For every $p_i$, we perform a linear regression fit, $f_i$, over the observations within that period of time where $f_i:E \rightarrow \mathbb{R}$. In practice, the overall regression function, $speed$, for a road segment becomes a piecewise linear function:
\begin{equation}\label{eq:speed-piecewise}
speed(e,t, w) =
\begin{cases}
f_1(e)       & \quad \text{if } t \in p_1\\
f_2(e)  & \quad \text{if } t \in p_2\\
&\vdots\\
f_n(e) & \quad \text{if } t \in p_n
\end{cases}
\end{equation}

where $f_1,...,f_n$ are linear regression functions for each partition over the observations for segment $e$. The idea is illustrated in Figure \ref{fig:segmented-regression}.
\begin{figure}[H]
\centering
\includegraphics[trim={0 8cm 0 9cm},clip, width=\textwidth]{figures/piecewise.pdf}
\caption{Example of piecewise linear regression - day of 24 hours are partitioned into 4 parts. Every partition has a linear function describing traffic in this time period.}
\label{fig:segmented-regression}
\end{figure}
% Why use it?
% - We want to predict impact of traffic on travel time.
% - How do we measure traffic ?
% 	-	Capacity of roads?
% 		- Difficult to determine accurately, people drive unsafe?
% 	- 	Speeds on roads?
% 	-	Try to classify on congestion?
% 		- k-ary class feature, that classifies a road.
% 		- Penalty of different classes.
% 		- How do we discretize our data into reasonable classes?
% 		- What about roads costs that are almost equal?
% 		- Classes defined by us might not reflect reality.
% 	-	Speed is more "pure" measurement of travel properties of a road
% 		- Speed tells both tells us the penalty of driving on road and has a better granularity of deciding between two almost equal costs of roads.
% 		- Methods for learning traffic:
% 			-	Linear regression
% 			-	Polynomial regression
% 				- Fits the learning data better, but might not fit future samples better! (SSE)
% 			-	But! Traffic also changes over time? (day, week etc..)
% 			-	Divide days into segments, for every segment define fit a linear regression function.
% 			-	How do we segment days?
% 			-	Residual sum
% 			-	Algorithms, M5P automatically splits and generates linear functions
%
% What is it?
% How did we use it?
% Alternatives ?

\subsection{Partitioning scheme}\label{patterns:segmentation}
% Why do it?
%	Want: piecewise linear functions, but how should we partition the days?
In order to use the piecewise linear regression approach, we must devise a partitioning scheme. One such scheme could be the one in Figure \ref{fig:segmented-regression} where every day a split into 4 equal partitions: night, morning, afternoon, evening. The following problems are raised by the choice of partitioning scheme:
\begin{itemize}
	\item Where should we split the day, such that important changes in traffic are captured?
	\item How many observations is required to perform regression?
	\item How do we guarantee enough observations in every partition?
\end{itemize}
While it makes sense to have equally sized partitions, it does not guarantee that there will be enough observations to perform an accurate regression for every partition (an example would be, that there are usually less vehicles driving at night, thus less observations in this partition). Furthermore, since traffic patterns are likely to be different for different road segments, such a static partitioning is not guaranteed to capture important patterns in traffic, such as morning traffic. Therefore, we must incorporate a finer granularity in the partitioning scheme, possibly different for every segment, to capture the differences in traffic patterns and to ensure that there will be enough observations to actually perform regression.

This leads to a dynamic partitioning scheme, where the partitioning is not explicitly defined, but inferred based on the density of observations and how well regressions fit the data. Such a scheme must have some measure to decide whenever to perform a partitioning. Several standard measures comes to mind, such as the absolute sum of error, the Sum of Squared Errors or Standard Deviation. The chosen measure is then used, to evaluate how possible partitions minimize the measure.
The dynamic partitioning in this system is based on M5' model trees utilising standard deviation, which is discussed further in \secref{patterns:weka}.

%\subsection{Quality measure}\label{quality-measure}
%Some of the observations might be based on a small number of GPS samples, or data that is map-matched with a high error rate (accuracy of which road was best to map-match on). Such observations could potentially introduce noise into the regression model and must therefore be taken into account when we construct the model trees.

%Furthermore, we are interested in weighing the most accurate data higher than data with worse quality, since more accurate data tells us more about the traffic on a given segment. To do that, we introduce a quality measure of observations. The quality measure describes the ratio between the number of GPS samples used to create an observations, and the length of the road segment where the observation was made. That is, the quality measure, $Q$ of an observation, $o$ is given by:
%\begin{equation}
%Q(o) = \frac{|wp|}{|e|}+0.29
%\end{equation}
%where $|e|$ is the length of a road segment $e$, and $|wp|$ is the number of waypoints used to construct $o$.
% Why?
% 	some data might be unreliable, better to use good data first
% What?
% 	We introduce a quality measure --> such that we can decide which observations are "better" in the sense of accuracy observations.
% How?
%	based on the ratio between the length of the segment and the
%	number 	of 	waypoints
% Could also include error rate from mapmatching, but unsure what it means (probably means accuracy of which the mapmatching was done)

\subsection{Model trees}\label{patterns:weka}
% Why use it? (does exactly what we need)
To implement the piecewise regression model described in \secref{patterns:regression-model}, we must either construct an algorithm or apply an existing one to the training dataset of observations. One could develop a partitioning scheme and then perform a linear regression on each partition. However, such an approach could potentially yield regression models that does not take into account the growth of predecessor and successor partitions' models. Furthermore, it is not clear how an effective partitioning scheme should be devised, to facilitate dynamic partitioning.

% What is it? (M5P algorithm)
Fortunately, existing work related to the piecewise linear regression model approach exists, in so called \emph{model trees} and \emph{regression trees}. As illustrated in \figref{fig:model-tree}, a \emph{model tree} is a decision tree that has regression models that can either be functions or constants at the leaves. A regression tree is a specialization of a model tree, where the regression models at the leaves are constants.

Several algorithms exist for inducing a piecewise linear regression including \emph{CART}, \emph{M5}, \emph{M5'} - an improved version of M5 - and \emph{MARS} \todo{need refs for these algorithms}. Wang and Witten\cite{IMTCC96} argues, that M5' performs somewhat better than M5 and produces model trees that are more comprehensible due to the size of the trees. Furthermore, open-source implementations of M5' are available for use, which provide a useful basis for inducing model trees for the dataset.

% How do we use it? (M5p in weka)
For implementing M5' model trees and data filters we use the machine learning tools and algorithm implementations in Weka\cite{Weka}. Weka contains an implementation of the M5' model tree induction algorithm (called M5P in Weka). Weka can be invoked directly from Java code, and therefore integrates well with the rest of the system.

\subsubsection{M5' model tree induction}
% How does it work?
M5' constructs a model tree, like the one illustrated in Figure \ref{fig:model-tree}, from a set of training examples, $T$.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
	\node {t}
	[level distance=3cm]
	child { node {$f_1(e)$} edge from parent [->] node [left] {$ < 06:00 $} }
	child { node {t}
		child { node {$f_2(e)$} edge from parent [->] node [left] {$ < 18:00 $}}
		child { node {$f_3(e)$} edge from parent [->] node [right] {$ 18:00 \le $} }
		edge from parent [->] node [right] {$06:00 \le $}
	}
	;
	\end{tikzpicture}
	\caption{Example of a simple model tree with three partitions and three linear regression models. The $t$ feature corresponds to the input time feature in equation \ref{eq:speed-piecewise}}
	\label{fig:model-tree}
\end{figure}
It works by recursively constructing a decision tree by splitting $T$ into subsets $T_1,T_2$ where the split point is determined by examining all possible splits over all non-target features (in our case, time) and the domains of those features. The split that yields the highest reduction in standard deviation is chosen at each step.
The resulting tree is then pruned for nodes and leaves that yield a worse standard deviation reduction than parent nodes. This pruning process makes sure the final model is not overfitting the training examples and can better be applied to future data.
Finally, the resulting regression models at the leaves of the tree are mutually smoothed as to minimizes sharp transitions from model to model.

Using this approach, M5' facilitates a dynamic partitioning scheme, based on the measure of standard deviation and smooths transitions between partitions.

\subsection{Generating speed functions}\label{sec:speedfunctiongen}
% Why ?
Using the M5' implementation in Weka, we generate model trees for road segments on a day-by-day basis. However, several issues are raised about the data used for inducing the model trees:
% What ?
\begin{itemize}
\item Noisy data can skewer the regression models to be inaccurate.
\item Incorrectly map-matched observations can skewer regression models.
\item Observed speeds above the speed limit introduces the possibility that the system can suggest driving faster than allowed.
\item A segment might not have enough observations to construct a (useful) regression model.
\end{itemize}
To minimize the effect of these issues we perform filtering on the observations and take measures to ensure the distribution of observations are fair, that is, before building model trees for a road segment, the distribution of observations throughout the day is somewhat balanced.
\subsubsection{Filtering the data}
The primary filtering is based on two quality measures called \emph{Median Sample Rate} (MSR) and \emph{Map-matching Error} (MME). MME is the error returned from the map-matching service, and MSR is computed when observations are created. The idea behind MSR is illustrated \figref{fig:MSR}.

The median sample rate represents the median of the change in time between the GPS samples a given observation is based on. 
In order to use MSR and MME to filter observations thresholds for these quality measures are required to be met by observations to be used in the regression models. The thresholds are determined from an empirical study of how high the requirements to MSR and MME could be without discarding too much data, since discarding too much data would yield an insufficient amount of observations to create accurate regression models. Table \ref{tab:datafiltering} shows different results for different thresholds.

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{figures/MSR.pdf}
	\caption{Median Sample Rate usage - The samples s1,s2,s3 are map-matched to the road segment between u and v. MSR is calculated as the median of r1, r2 which represents the difference in time between samples.}
	\label{fig:MSR}
\end{figure}

%  SELECT COUNT(o.*) FROM Observations o, Segments s, Ways w, Roadtypes t WHERE o.speed > 1 AND o.speed < t.speedlimit * 2 AND o.length > 0.005 AND o.mediansamplerate <= 20 AND o.error < 100 AND numwaypoints > 4 AND o.segment = s.id AND s.way = w.id AND w.type = t.id;
\begin{table}[h]
	\centering
	\begin{tabular}{TlTlTlTlTlTlTlT}
		\thickhline
		\textbf{Speed}                                                                          & \textbf{Length}  & \textbf{MSR}  & \textbf{MME} & \textbf{No. wpts} & \textbf{Observations} & \textbf{\%} \\ \thickhline
		-                                                                                       & -                & -             & -              & -               & 4584306             & 100 \\ \thickhline
		\begin{tabular}[c]{@{}l@{}}\textgreater 1 km/h\\ \textless speed limit * 2\end{tabular} & \textgreater 5 m & \textless= 20 & \textless 100  & \textgreater= 5 & 269527              & 5.88 \\ \thickhline
		\textless= speed limit                                                                  & -                & \textless= 20 & \textless 100  & -               & 861766              & 18.8 \\ \thickhline
		\begin{tabular}[c]{@{}l@{}}\textgreater 1 km/h\\ \textless speed limit * 2\end{tabular} & \textgreater 5 m & \textless= 20 & \textless 100  & -               & 867128              & 18.9 \\ \thickhline
		\begin{tabular}[c]{@{}l@{}}\textgreater 1 km/h\\ \textless speed limit * 2\end{tabular} & -                & \textless= 20 & \textless 100  & -               & 878426              & 19.2 \\ \thickhline
		\textless speed limit * 2                                                               & -                & \textless= 20 & \textless 100  & -               & 898774              & 19.6 \\ \thickhline
		\begin{tabular}[c]{@{}l@{}}\textgreater 1 km/h\\ \textless speed limit * 2\end{tabular} & \textgreater 5m  & \textless= 60 & \textless 100  & \textgreater= 3 & 913779              & 19.9 \\ \thickhline
		\rowcolor[HTML]{34FF34}
		\begin{tabular}[c]{@{}l@{}}\textgreater 1 km/h\\ \textless speed limit * 2\end{tabular} & \textgreater 5m  & \textless= 60 & \textless 100  & -               & 1715070             & 37.4 \\ \thickhline
		\textless= speed limit                                                                  & -                & \textless 300 & \textless 100  & -               & 2861949             & 62.4 \\ \thickhline
	\end{tabular}
	\caption{Observations remaining after using various filters.}
	\label{tab:datafiltering}
\end{table}

\textbf{No. wpts} is the total number of waypoints on the link (recall that a link is the entity to which a set of GPS sample is matched), \textbf{Length} is the distance driven between the first and the last waypoint in the link, and \textbf{Observations} is the number of observations remaining after applying the filters.
The chosen threshold combination is highlighted with green. This combination strikes a fair balance between MSR and MME and amount of filtered observations.

Furthermore observations with speeds higher than two times the speed limit are filtered, because such high speeds are likely to be noise data. Such speeds are possible, since the speeds of observations are estimates based on changes in time and location. Observations with speeds between 100\% and 200\% of the speed limit are lowered to the given segment's speed limit, since such observations might be realistic driving, but we do not want speeding drivers to affect the resulting regression model.

\subsubsection{Distribution of data}\label{datadistibution}
With the filtered observations we are creating speed functions for every day of the week for all the segments. But since the remaining dataset of observations covers a small percentage of the segments on all days, there will be some segments that do not have enough observations to create a solid foundation to run the M5P algorithm. Because of this shortage of observations we have decided to use two ways of adding observations to a given segment.

First, we look at the observations for the same day of neighbouring segments of the same road type. The reason that we are looking to neighbouring segments is that they often reflect the traffic situation of the given segment. If neighbouring segments do not have additional information to add to a segment, then we are looking at the observations for the segment on other days. Here we are differentiating between weekdays, and weekends, which we have classified as weekdays being Monday, Tuesday, Wednesday, Thursday, and Friday; weekends are the rest. The reason for this distinction is that we, based on our experience, believe that some traffic patterns can be reasonably divided into these groups, such as weekday traffic when going to and from work.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/intervals.png}
\caption{Example of intervals with 5 observations in each interval}
\label{fig:intervals}
\end{figure}

Besides looking for more observations we are also figuring out which of these observations to add to ensure that most of the day is covered by observations. The way that we check if there are enough observations, are by splitting the observations into intervals of 6 hours, as seen in \figref{fig:intervals}, for each of these intervals we check that there are at least k observations. We have not yet found the best k value, but this is one area that is possible to look at in future work.

Additionally, we strive to add observations such that the 25th percentile of observation times is at 6 o'clock, the 50th is at 12 o'clock, and the 75th is at 18 o'clock. The reasoning for using this way to ensure that the data is uniformly distributed has shown not to be the best idea since we will accept a segment to be processed by M5P if it just have k observations in each interval, and the 25th, 50th, and 75th percentile are fairly close to the optima. This does not prevent observations from cluttering, i.e. we are not guaranteed a good distribution.

%We are also looking at the 25, 50 and 75 percentile if these values are laying in between a treshold of 6, 12 and 18. If this is not the case additional observations are added to move these points towards the values. The reasoning for using this way to ensure that the data is uniformly distributed, have shown not to be the best idea since we will accept a segment to be proccessed by M5P if it just have k observations en each interval, and the 25, 50 and 75 percentile is to some point satisfied. This does not hinder that observations can just clutter at some points, and therefore they are not uniformly distributed.
