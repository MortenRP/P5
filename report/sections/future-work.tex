Several issues can be found with the current state of the system. Therefore, many interesting areas of improvement to investigate further exist. In this chapter we consider current issues and discuss improvements that can be made to the system in order to alleviate the issues about the data, preprocessing, regression models, routing and client-server features.

\section{Data acquisition}
The current speed of observations are an estimate based on changes locations and time over a GPS trajectory. This estimate is not really precise, which can affect the final traffic predictions. Furthermore, errors in GPS samples propagate through to the speed estimation, resulting in worse estimations. A direct measurement of the speed travelled in the dataset would be very useful and improve the accuracy of the prediction model.

Unfortunately, the dataset used contained only timestamped GPS coordinates, which in turn also had a high variance in sampling rate. In GPS samples with low sampling rate, the accuracy of the speed predictions suffered even more because of the uncertainty of the speed.

By collecting data through the client of the system, we can include the speed directly from sensors in the client devices. Furthermore, we gain a better sampling rate, which would yield a more useful dataset.

Alternatively, the speed estimation could be done using tested approaches. Nermin\cite{Mudzelet07} proposes a method for calculating travel times on road networks using GPS data, by calculating speeds over multi passes. Speeds for roads with sufficient data are calculated first, directly from GPS samples. The second pass handles roads with insufficient data, where the speed estimated from neighbouring roads which speeds are calculated from the first pass. This approach uses five passes, and would probably yield a better speed estimation than the method currently implemented.

This approach could also be utilized for checking the distribution of data. We could also have been working on this data set by running multi pass over it. This could have been done such that in the first pass only processed segments that contained enough observations, and which were to some degree uniformly distributed over the whole day. Then for the second pass process the segments which now also had neighbours of the same road type, that now had speed functions, and then use these as a part of determining the speed functions of these segments. Then at the last pass, if any segments were left then we could have used the median speed of all segments of the same road type to determine those speed functions.

%\section{Data gathering}
%For this project to work properly, the data needed is a big part of it. Some of the problems with the data used in this project are described in section \ref{sec:datafiltering}. For the system to be able to make good decision a lot of data is needed and should be well distributed over all routes. It is hard to tell when there is enough data for each route.

%To collect this data a smart and easy way needs to be in play. In section \ref{sec:existingwork} some of the possible way of collecting data is analyse. The data should consist of location, time and speed, location and time is in the data we got now but we need to calculate the speed form the data we got now. Since there can be some error with the location data then the speed calculation will yield a wrong result, and the cost function is depended on the correctness of the speed.
%The client-server solution we proposed is set up to collect the data needed, if we had more time for this project we could have collected data on our own and had a lot better data to work with, since the data we can collect through the client would be the GPS coordinates, time and speed, where speed has been a huge issue for us because it was not given in the dataset we worked with.

%Instead of checking the distribution of data as explained in \ref{datadistibution}, we could also have been working on this data set by running multi pass over it. This could have been done such that in the first pass only processed segments that contained enough observations, and which were to some degree uniformly distributed over the whole day. Then for the second pass process the segments which now also had neighbours of the same roadtype, that now had speed functions, and then use these as a part of determining the speed functions of these segments. Then at the last pass, if any segments were left then we could have used the median speed of all segments of the same roadtype to determine those speed functions.\todo{ref til aau artikel}

\section{Data preprocessing}
The current approach for data filtering seems ad-hoc. Many of the difficulties with estimating speed comes from errors in the GPS data, which could potentially be minimized by taking a more systematic approach for filtering the data. It would be interesting to investigate such methods for filtering noise data much earlier in the system.

Likewise, we use an external service (TrackMatching) to perform the map-matching process, and instead focus on the prediction of traffic. Although the TrackMatching API yields matches that seem reasonable, it is not clear how the algorithms for map-matching in this service work, which could provide information about the performance of the algorithms. The only measurement of accuracy for a given match has been an error measurement. Unfortunately the meaning of the error measure is not documented, and so we can only guess about the impact of the different values it takes. 

Furthermore, it was discovered that TrackMatching makes heavy use of inference of roads, if GPS sample rate is very low. In some cases, this inference yielded matches to many road segments that actually had been no GPS samples on. One could argue that this is not the best approach, since inferring so many roads just to be able to give results is not justifiable.

More closely evaluating alternative map-matching tools or implementing a map-matching algorithm ourselves would not only give more control of the map-matching process, but also give more insight into accuracy of matches to reason about the quality of GPS samples.
%Map-matching as describe in section \ref{sec:mapmatching} looks at the way this project have implemented this process, but this have highlighted some problems. Given a set of way-points the mapping function will give a route if one or more of these way-points are wrong the function will still make a route with them, never looking if this is possible.

%This means that the map data need to look at again to fix these errors. A solution would be to build the map-matchere ourselves "it was out of this project scope". It would allow us the merge the mapping and error detection into one process. Another possible solution could be to use another map-matching program, but more time would be need to make an informed decision on which of the service from section \ref{sec:mapmatching} to or if non of them are usable.

\section{Regression model}
The current regression models includes only the time as the feature to predict speed on. It would be interesting to explore how other features of traffic could be used to predict the speed. Features such as the day and minimum and maximum speed driven could be used, which would lead to the system learning patterns across days in the models e.g.  that traffic might be similar in work-days, and therefore is no reason to have seperate regression models for these days. 

As the current dataset used for learning the regression models are collected over seven days, the data is fairly recent and reliable. However if the system was to be used, the data collected by clients would be used to continiously learn the regression models. This raises the issue of which data should be considered more reliable and how much it should weigh in predicting traffic patterns. It seems most reasonable that the most recent data is more reliable than older data. This means that some measure of decay of the data must be introduced, to distinguish between recent and older data weights in the regression models.

\section{Routing}
The current graph traversal running time can be improved in several ways. By using the techniques bi-directional search and Contraction Hierarchies it would improve the query time for route finding.

Preprocessing the road network graph using contraction hierarchies, a new contracted graph is created. The edges of the original graph is contracted into so called “shortcut” edges, that are aggregate edges of  several other edges. The weight of a shortcut is the sum of its constituent edges. This contraction process reduces the number of nodes a graph search algorithm has to traverse during a query, which in turn improves the query time. 
Originally, contraction hierarchies are used for static graph networks, where the edges do not change. However, in the knowledge representation in this project, the edges change based on the day and time of day. Existing work on  contraction hierarchies in dynamic road networks exists,  such as \emph{KaTCH}\cite{KaTCH}\cite{KaTCHi} and would be possible to implement in the project in the future since this implementation assumes exactly a piecewise function as weight functions for edges.

As the name implies, bi-directional search, traverses the graph concurrently (or in parallel) in a forward direction from start-to-goal node, and in the backward direction from goal-to-start node. By using bi-directional search the query time can in theory be decreased by half, which is certainly an improvement.

\section{Client-Server features}
There are several things that can be added to the client and the server. For the client, a specific route option can be added, where you are able to send your current position and where you want to go, and get a route and estimated travel time. A voice like there exists on other GPS, which tells you what direction to turn and if you have made a wrong turn.

Another thing would be live updates, as in if the traffic conditions have changed which would make the server send a new route to the client. When given a route the server could also give two alternative routes with the time differences and distance, like Google Maps does.
When the client asks for a route the server could respond with three routes, which ordinary GPS does, the fastest, the shortest and the cheapest regarding  mileage.

At the moment if a message is lost the message is just discarded, which does not have any impact if it is just a few messages, but if there are a lot of messages lost this could potentially become a problem. Therefore it could be an idea to handle lost messages more appropriate, like cashing all the lost messages and then send them all together at some point.

At the moment the client only works on Android, it would of course be advantageous to have the client working on iOS, Blackberry, and Windows smartphones. Also, we have only tested that the server can handle five clients, but a realistic use of the system would involve many more clients.